{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark MLlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timer(f):\n",
    "    def tmp(*args, **kwargs):\n",
    "        t = time.time()\n",
    "        res = f(*args, **kwargs)\n",
    "        print(\"Время выполнения функции {}: {}\\n\".format(f.__name__, time.time()-t))\n",
    "        return res\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_COLLECTION_PASSES = 10\n",
    "NUM_TOPICS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения функции prepare_corpus: 93.01701879501343\n",
      "\n",
      "Время выполнения функции fit_spark_mllab: 221.61254501342773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@timer\n",
    "def prepare_corpus(file_path):\n",
    "    sc = SparkContext(appName=\"LDA\")\n",
    "    data = sc.textFile(file_path)\n",
    "    parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "    return sc, parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "    \n",
    "\n",
    "@timer\n",
    "def fit_spark_mllab(sc_corpus, maxIterations, num_topics):\n",
    "    ldaModel = LDA.train(sc_corpus[1], maxIterations=maxIterations, k=num_topics)\n",
    "    sc_corpus[0].stop()\n",
    "    \n",
    "fit_spark_mllab(prepare_corpus(\"pubmed.txt\"), NUM_COLLECTION_PASSES, NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigARTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import artm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def prepare_corpus(collection_name):\n",
    "    return artm.BatchVectorizer(data_path='.', data_format='bow_uci',\n",
    "                                collection_name=collection_name, target_folder= collection_name+'_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def fit_artm(batch_vectorizer, num_topics, num_collection_passes):\n",
    "    lda = artm.LDA(num_topics=num_topics, alpha=0.01, beta=0.001, cache_theta=True,\n",
    "                   num_document_passes=5, dictionary=batch_vectorizer.dictionary)\n",
    "    lda.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=num_collection_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения функции prepare_corpus: 123.62481212615967\n",
      "\n",
      "Время выполнения функции fit_artm: 167.00855827331543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_artm(prepare_corpus('pubmed'), NUM_TOPICS, NUM_COLLECTION_PASSES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
